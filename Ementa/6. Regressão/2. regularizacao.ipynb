{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac626808",
   "metadata": {},
   "source": [
    "# Ridge Regression (L2)\n",
    "Sabemos que a otimização por  *least squares* trata-se basicamente de minimizar a seguinte expressão:\n",
    "\n",
    "$$RSS = e_1^{2} + e_2^{2} + \\ldots + e_n^{2} = \\sum_{i = 1}^{n} e^{2}_{i}$$\n",
    "\n",
    "Entretanto, com ridge regression, tenta-se estimar coeficientes $\\beta^{R}$ que minimizem:\n",
    "\n",
    "$$Ridge = RSS + \\lambda \\sum_{k = 1}^{p}\\beta^{2}_{k}$$\n",
    "\n",
    "onde $\\lambda \\geq 0$ é um *tuning parameter*, para ser determinado separadamente. Assim como os mínimos quadrados, o *ridge regression* busca encontrar coeficientes que minimizem o $RSS$. Entretanto, o segundo termo, $\\lambda \\sum_{k = 1}^{p}\\beta^{2}_{k}$ chamado de *shrinkage penalty* é pequeno quando $\\beta_1, \\beta_2, \\ldots, \\beta_p$ são próximos de zero, ou seja, minimizar a função envolve minimizá-los também de encontro a zero.\n",
    "\n",
    "O parâmetro $\\lambda$ serve para controlar o impacto desses dois termos nos coeficientes da regressão. Quando $\\lambda = 0$ então o impacto é nulo e o resultado será o mesmo produzido com *least squares* normalmente. Entretanto, conforme $\\lambda \\to \\infty$ o impacto do *shrinkage* aumenta e os coeficientes $\\beta_k$ tendem a zero\n",
    "\n",
    "## Qual a vantagem do Ridge Regression?\n",
    "A vantagem dele sobre *least squares* comum está no *trade-off* entre viés e variância. Conforme $\\lambda$ aumenta, a variãncia diminui mas o bias aumenta. Ele funciona melhor em casos que o *least squares* simples tem muita variância. Além disso, o custo computacional dele é muito reduzido em relação a casos em que precisamos realizar $2^p$ combinações e, foi demonstrado que as computações necessárias para resolver *simultãneamente para todos os valores de $\\lambda$ são quase idênticas ao tentar resolver o *least squares* simples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c3750e",
   "metadata": {},
   "source": [
    "# The Lasso (L1)\n",
    "O Ridge Regression tem suas desvantagens, pois diferentemente dos métodos explicados antes dele, ele envolve **todas** as variáveis explicativas, sendo que as menos importantes tendem a ter coeficiente zero, mas nenhuma é efetivamente zerada.\n",
    "A expressão dada pelo *Lasso Regression* é similar ao *Ridge*, como apresentado abaixo:\n",
    "\n",
    "$$RSS = e_1^{2} + e_2^{2} + \\ldots + e_n^{2} = \\sum_{i = 1}^{n} e^{2}_{i}$$\n",
    "$$Lasso = RSS + \\lambda \\sum_{j}^{p} |\\beta_j|$$\n",
    "\n",
    "A diferença se dá apenas por se utilizar de $|\\beta_j|$ ao invés de $\\beta_j^2$ (Ridge). Dessa forma, é possível dizer que o Lasso realiza uma **penalidade** $l_1$ ao invés de $l_2$ como no Ridge\n",
    "\n",
    "**Definição da norma $l_1$:**\n",
    "    $$||\\beta||_2 = \\sum_{j}|\\beta_j|$$\n",
    "\n",
    "**Definição da norma $l_2$:**\n",
    "    $$||\\beta||_2 = \\sqrt{\\sum_{j}\\beta_j^2}$$\n",
    "\n",
    "Assim como no ridge regression, o lasso comprime os coeficientes de forma \n",
    "a tender a zero, entretanto no caso da penalidade $l_1$, ela tem o efeito de\n",
    " forçar alguns coeficientes para serem exatamente iguais a zero quando o parâmetro\n",
    "  $\\lambda$ é suficientemente grande. Portanto, de certa forma, ela também performa uma certa **seleção de variáveis**, gerando modelos muito mais fáceis de interpretar do que os que utilizam ridge regression.\n",
    "\n",
    "## Como definir o $\\lambda$ ?\n",
    "Uma forma, seria escolher um grid de valores $\\lambda$ e é computado o erro com **cross-validation** para cada valor de $\\lambda$ e é selecionado o valor de $\\lambda$ que teve o menor erro com validação cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bebcbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear: R² = 0.644\n",
      "Coeficientes: [ 0.54307245  0.37925576  0.20599436 -0.0105318  -0.04046648]\n",
      "Lasso (L1): R² = 0.554\n",
      "Coeficientes: [ 0.41183971  0.28334694  0.0981355  -0.         -0.        ]\n",
      "Ridge (L2): R² = 0.644\n",
      "Coeficientes: [ 0.54226545  0.37873843  0.20566955 -0.01052715 -0.04048081]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Dados com mais features\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 5)\n",
    "y = 2 * X[:,0] + 1.5 * X[:,1] + 0.5 * X[:,2] + np.random.randn(100) * 0.5\n",
    "\n",
    "# Split e scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Modelos\n",
    "models = {\n",
    "    'Linear': LinearRegression(),\n",
    "    'Lasso (L1)': Lasso(alpha=0.1),\n",
    "    'Ridge (L2)': Ridge(alpha=0.1)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    score = r2_score(y_test, y_pred)\n",
    "    print(f\"{name}: R² = {score:.3f}\")\n",
    "    \n",
    "    if hasattr(model, 'coef_'):\n",
    "        print(f\"Coeficientes: {model.coef_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
